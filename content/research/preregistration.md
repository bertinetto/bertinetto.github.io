---
title: "Workshop series on Pre-registration in Machine Learning"
conference: "NeurIPS and ICCV"
year: 2020
show_year: false
arxiv: "https://preregister.science/neurips2020.html"
authors: ["Co-organised with João F. Henriques, Samuel Albanie, Michela Paganini, Gül Varol"]
thumbnail_width: "0%"
highlight: false
tldr: "Benchmarks on popular datasets have played a key role in the considerable measurable progress that machine learning has made in the last few years. But reviewers can be tempted to prioritize incremental improvements in benchmarks to the detriment of other scientific criteria, destroying many good ideas in their infancy. Authors can also feel obligated to make orthogonal improvements in order to “beat the state-of-the-art”, making the main contribution hard to assess.

Pre-registration changes the incentives by reviewing and accepting a paper before experiments are conducted. The emphasis of peer-review will be on whether the experiment plan can adequately prove or disprove one (or more) hypotheses. Some results will be negative, and this is welcomed. This way, good ideas that do not work will get published, instead of filed away and wastefully replicated many times by different groups. Finally, the clear separation between hypothesizing and confirmation (absent in the current review model) will raise the statistical significance of the results."
---
