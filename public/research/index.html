<!DOCTYPE html>
<html lang="en-gb">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Luca Bertinetto personal page/research/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/terminal-0.7.4.min.css">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/animate-4.1.1.min.css">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/console.css"><style>
:root {
    --base-font-size: 1.05rem;
    --header-font-size: 1.15rem;
}

html, body {
    font-size: var(--base-font-size);
}

 
.terminal-nav, 
.terminal-logo, 
.terminal-menu, 
.logo,
.terminal-prompt,
.terminal-menu li a,
h1, h2, h3 {
    font-size: var(--header-font-size) !important;
}

 
.terminal-prompt a,
.terminal-menu > ul {
    font-size: var(--header-font-size) !important;
}

 
.research-item h1 {
    font-size: var(--header-font-size);
}

 
@media (max-width: 768px) {
    .thumbnail {
        display: none !important;
    }
    .content-wrapper {
        gap: 0 !important;
    }
}

.profile {
    height: 300px;
    background-size: cover;
    background-position: center;
    margin-bottom: 20px;
}

.info {
    padding: 20px;
}

.social li {
    list-style: none;
    padding: 0;
    margin: 10px 0;
}

.social li a i {
    margin-right: 5px;
}
</style> 
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       
      <link href="http://localhost:1313/research/index.xml" rel="alternate" type="application/rss+xml" title="Luca Bertinetto personal page" />
    <meta property="og:title" content="Research" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:1313/research/" />


<meta name="twitter:title" content="Research"/>
<meta name="twitter:description" content=""/>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              <a href="http://localhost:1313/" class="no-style site-name">Luca Bertinetto personal page</a>:~# 
              <a href='http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research/http://localhost:1313/research'>research</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="http://localhost:1313/about/" typeof="ListItem">about/</a></li>
                
                <li><a href="http://localhost:1313/posts/" typeof="ListItem">posts/</a></li>
                
                <li><a href="http://localhost:1313/photos/" typeof="ListItem">photos/</a></li>
                
                <li><a href="http://localhost:1313/research/" typeof="ListItem">research/</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container animated zoomIn fast" >
        
<div class="wrapper">
  <div class="content" style="max-width: 100%; margin: 0;">
    <h1>Research</h1>
    <br/>
    

    <style>
      @media (max-width: 768px) {
        .thumbnail {
          display: none !important;
        }
        .content-wrapper {
          gap: 0 !important;
        }
      }
    </style>

    <div class="research-list">
      
      <div class="research-item" >
        <h1>
          <span style="font-weight: normal;">[ICLR 2022]</span>
          <a href="https://arxiv.org/abs/1606.05233" title="Learning feed-forward one-shot learners">Learning feed-forward one-shot learners</a>
        </h1>
        <div class="content-wrapper" style="display: flex; gap: 20px;">
          
          <div class="thumbnail" style="flex: 0 0 25%;">
            
              <img src="/images/learnet.png" alt="Learning feed-forward one-shot learners" style="width: 100%; height: auto;">
            
          </div>
          <div class="details" style="flex: 1;">
            <p>One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.</p>
          </div>
        </div>
      </div>
      <hr>
      
      <div class="research-item" >
        <h1>
          <span style="font-weight: normal;">[CVPR (oral) 2022]</span>
          <a href="https://arxiv.org/abs/1606.05233" title="Learning feed-forward one-shot learners">Learning feed-forward one-shot learners</a>
        </h1>
        <div class="content-wrapper" style="display: flex; gap: 20px;">
          
          <div class="thumbnail" style="flex: 0 0 25%;">
            
              <img src="/images/learnet.png" alt="Learning feed-forward one-shot learners" style="width: 100%; height: auto;">
            
          </div>
          <div class="details" style="flex: 1;">
            <p>One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.</p>
          </div>
        </div>
      </div>
      <hr>
      
      <div class="research-item" >
        <h1>
          <span style="font-weight: normal;">[CVPR 2019]</span>
          <a href="https://arxiv.org/abs/1606.05233" title="Learning feed-forward one-shot learners">Learning feed-forward one-shot learners</a>
        </h1>
        <div class="content-wrapper" style="display: flex; gap: 20px;">
          
          <div class="thumbnail" style="flex: 0 0 25%;">
            
              <img src="/images/learnet.png" alt="Learning feed-forward one-shot learners" style="width: 100%; height: auto;">
            
          </div>
          <div class="details" style="flex: 1;">
            <p>One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.</p>
          </div>
        </div>
      </div>
      <hr>
      
      <div class="research-item" style="background-color: #fff3e0; padding: 15px; border-radius: 5px;">
        <h1>
          <span style="font-weight: normal;">[ECCV workshops 2016]</span>
          <a href="https://arxiv.org/abs/1606.09549" title="Fully-convolutional siamese networks for object tracking">Fully-convolutional siamese networks for object tracking</a>
        </h1>
        <div class="content-wrapper" style="display: flex; gap: 20px;">
          
          <div class="thumbnail" style="flex: 0 0 25%;">
            
              <img src="/images/siamfc.png" alt="Fully-convolutional siamese networks for object tracking" style="width: 100%; height: auto;">
            
          </div>
          <div class="details" style="flex: 1;">
            <p>This work introduced a real-time object tracking framework using fully-convolutional Siamese (aka contrastive) networks trained offline on large-scale video data. By learning a generic similarity function between exemplar templates and search regions, the method eliminated traditional online model adaptation while achieving very competitive results at high framerates (~80 fps). Its key innovation – dense cross-correlation via bilinear layers for efficient sliding-window evaluation – became foundational for subsequent real-time trackers. The demonstration that deep similarity learning could generalize across video domains without test-time fine-tuning influenced the tracking community&#39;s shift toward offline-trained architectures, establishing an important baseline for balancing accuracy and speed in visual tracking systems.</p>
          </div>
        </div>
      </div>
      <hr>
      
      <div class="research-item" >
        <h1>
          <span style="font-weight: normal;">[NeurIPS 2016]</span>
          <a href="https://arxiv.org/abs/1606.05233" title="Learning feed-forward one-shot learners">Learning feed-forward one-shot learners</a>
        </h1>
        <div class="content-wrapper" style="display: flex; gap: 20px;">
          
          <div class="thumbnail" style="flex: 0 0 25%;">
            
              <img src="/images/learnet.png" alt="Learning feed-forward one-shot learners" style="width: 100%; height: auto;">
            
          </div>
          <div class="details" style="flex: 1;">
            <p>One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.</p>
          </div>
        </div>
      </div>
      <hr>
      
    </div>
  </div>
</div>

        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
